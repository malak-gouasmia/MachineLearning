{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CS SIQ2-SIL2 TP02. Régression logistique Multi-classes\n",
    "\n",
    "Dans ce TP, nous allons généraliser la réression linéaire binaire afin de traiter le cas de multiples classes.\n",
    "Ensuite, dans la partie analyse, nous allons voir quelques méthodes pour traiter le classement multi-classes (cas d'étude, régression logistique)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Binôme 01** : GOUASMIA MALAK\n",
    "- **Binôme 02** : TOUHAR Afnane\n",
    "- **Groupe** : SIL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.21.5', '1.4.4', '3.5.2')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing          import Tuple, List, Type\n",
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTRODUCTION**\n",
    "\n",
    "Nous avons implémenté le cas d'une seule classe (binaire : oui ou non). \n",
    "Pour appliquer un classement sur plusieurs classes, nous pouvons entraîner $L$ modèles de régression logistique (où $L$ est le nombre des classes). \n",
    "Dans ce cas, nos résultats (Y) doivent encodée en 0 et 1. \n",
    "Pour un modèle $M_i$ d'une classe $C_i$, la sortie $Y$ doit avoir 1 si $C_i$, 0 si une autre classe. \n",
    "Cette architecture est appelée : One-to-rest classification.\n",
    "\n",
    "Une autre approche (celle que nous allons implémenter) est d'encoder la sortie en utilisant OneHot encoder. \n",
    "Pour $L$ classes et un échantillon donnée, nous allons avoir $L$ sorties (une ayant 1 et les autres 0). \n",
    "Pour un dataset avec $M$ échantillons, $N$ caractéristiques et $L$ classes, nous allons avoir les dimensions suivantes : \n",
    "- $X [M, N]$\n",
    "- $Y [M, L]$\n",
    "- $\\theta [N, L]$\n",
    "\n",
    "Cette dernière approche s'appelle maximum entropy (MaxEnt). \n",
    "C'est une généralisation de la régresion logistique binaire.\n",
    "\n",
    "\n",
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie sert à améliorer la compréhension des algorithmes d'apprentissage automatique vus en cours en les implémentant à partir de zéro. \n",
    "Pour ce faire, nous allons utiliser la bibliothèque **numpy** qui est utile dans les calcules surtout matricielles.\n",
    "\n",
    "### I.1. Combinaison linéaire\n",
    "\n",
    "Les $N$  caractéristiques sont combinées linéairement comme dans la régression linéaire binaire. \n",
    "La seule différence est que nous avons plus de classes, donc le nombre des paramètres va être multiplié par le nombre des classes.\n",
    "La somme pondérée d'une classe $c$ est calculée selon la formule : \n",
    "\n",
    "$$Z_c = zfn_c(X, \\theta) = \\sum\\limits_{j=0}^{N} \\theta_{(c, j)} X_j | X_0 = 1 $$\n",
    "\n",
    "La forme matricielle de $Z$ sera : \n",
    "$$Z = zfn(X, \\theta) = X \\cdot \\theta$$\n",
    "\n",
    "- $X[M, N]$      : une matrice de M lignes (échantillons) et N colonnes (caractéristiques, y compris le biais).  \n",
    "- $\\theta[N, L]$ : une matrice de N lignes (caractéristiques, y compris le biais) et L colonnes (classes). \n",
    "- $Z[M, L]$      : une matrice de M lignes (échantillons) et L colonnes (classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. ],\n",
       "       [0.5, 0.1, 0.6],\n",
       "       [0.2, 0.3, 0. ],\n",
       "       [0.7, 0.4, 0.6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Combinaison linéaire \n",
    "def zfn(X: np.ndarray, Theta: np.ndarray) -> np.ndarray: \n",
    "    return np.dot(X,Theta)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0. , 0. , 0. ],\n",
    "#        [0.5, 0.1, 0.6],\n",
    "#        [0.2, 0.3, 0. ],\n",
    "#        [0.7, 0.4, 0.6]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X_tn = np.array([[0., 0.], \n",
    "                 [1., 0.], \n",
    "                 [0., 1.], \n",
    "                 [1., 1.]]) # 4 échntillons, 2 caractéristiques\n",
    "Theta_tn = np.array([[0.5, 0.1, 0.6],\n",
    "                     [0.2, 0.3, 0.0]]) # 2 caractéristiques, 3 classes\n",
    "zfn(X_tn, Theta_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Calcul des probabilités\n",
    "\n",
    "Les valeurs combinées sont transformées à des probabilités en utilisant la fonction softmax. \n",
    "La fonction softmax nous assure que la somme des probabilités des classes soit égale à 1.\n",
    "Cette fonction prend les combinaisons linéaires $Z[M, L]$ et calcule les probabilités $P[M, L]$ comme suite : \n",
    "\n",
    "$$softmax(Z)=\\frac{e^Z}{\\sum\\limits_{k=1}^{L} e^{Z_k}}$$\n",
    "\n",
    "- $M$ : nombre des échantillons\n",
    "- $N$ : nombre des caractéristiques\n",
    "- $L$ : nombre des classes\n",
    "- La somme des probabilités de chaque ligne doit être 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.36029662, 0.24151404, 0.39818934],\n",
       "       [0.34200877, 0.37797814, 0.28001309],\n",
       "       [0.37797814, 0.28001309, 0.34200877]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Softmax\n",
    "def softmax(Z: np.ndarray) -> np.ndarray:\n",
    "    exp_Z = np.exp(Z)\n",
    "    sum_exp_Z = np.sum(exp_Z, axis=1, keepdims=True)\n",
    "    return exp_Z / sum_exp_Z\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0.33333333, 0.33333333, 0.33333333],\n",
    "#       [0.36029662, 0.24151404, 0.39818934],\n",
    "#       [0.34200877, 0.37797814, 0.28001309],\n",
    "#       [0.37797814, 0.28001309, 0.34200877]])\n",
    "#---------------------------------------------------------------------\n",
    "Z_tn = np.array([[0. , 0. , 0. ],\n",
    "                 [0.5, 0.1, 0.6],\n",
    "                 [0.2, 0.3, 0. ],\n",
    "                 [0.7, 0.4, 0.6]])\n",
    "softmax(Z_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Prédiction \n",
    "\n",
    "Etant donnée les probabilités des classes pour chaque échantillon, nous devons choisir la classe avec le max de probabilité.\n",
    "\n",
    "$$\n",
    "\\hat{C}^{(i)}_j = \n",
    "\\begin{cases}\n",
    "1 & si & H^{(i)}_j \\ge \\max P^{(i)}\\\\\n",
    "0 & sinon & \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $H[M, L]$ probabilités où chaque ligne est un échantillon et chaque colonne est une classe\n",
    "- $\\hat{C}[M, L]$ prédictions où chaque ligne est un échantillon et chaque colonne est une classe. $\\hat{C}^{(i)}_j \\in \\{0, 1\\}$\n",
    "\n",
    "Lorsqu'il y a deux colonnes ou plus ayant le même max, nous prenons la première."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Prédictions multiclasses\n",
    "def cn(H: np.ndarray) -> np.ndarray:\n",
    "    #indice de la colonne avec la valeur maximale pour chaque ligne\n",
    "    max_indices = np.argmax(H, axis=1)\n",
    "    # Initialiser un tableau de prédictions avec des zéros\n",
    "    C = np.zeros_like(H)\n",
    "    # Mettre à 1 l'élément correspondant à l'indice trouvé\n",
    "    C[np.arange(len(C)), max_indices] = 1\n",
    "    return C.astype(int)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[1, 0, 0],\n",
    "#        [0, 0, 1],\n",
    "#        [0, 1, 0],\n",
    "#        [1, 0, 0]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "             [0.36029662, 0.24151404, 0.39818934],\n",
    "             [0.34200877, 0.37797814, 0.28001309],\n",
    "             [0.37797814, 0.28001309, 0.34200877]])\n",
    "cn(H_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Calcul du coût \n",
    "\n",
    "Nous référons aux probabilités calculées par la fonction softmax comme $H$, où $H_c$ est la probabilité d'une classe $c$.\n",
    "Etant donné un échantillon $X^{(i)}$, son coût est calculé comme : \n",
    "\n",
    "$$ cout(H^{(i)}, Y^{(i)}) = - \\sum\\limits_{c=1}^{L} Y^{(i)}_c \\log(H^{(i)}_c)$$\n",
    "\n",
    "Le coût total est la moyenne des coût de tous les échantillons\n",
    "\n",
    "$$J(H, Y) = \\frac{1}{M} \\sum\\limits_{i=1}^{M} cout(H^{(i)}, Y^{(i)})$$\n",
    "\n",
    "- $H[M, L]$ : les probabilités estimées de chaque échantillon (M) de chaque classe (L)\n",
    "- $Y[M, L]$ : les probabilités réelles (1 ou 0) de chaque échantillon (M) de chaque classe (L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1913194530574498"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Coût du classement multiclasses \n",
    "def jn(H: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    M=len(Y)\n",
    "    return  np.mean( -np.sum(Y*np.log(H)))/M\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 1.1913194530574498\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "                 [0.36029662, 0.24151404, 0.39818934],\n",
    "                 [0.34200877, 0.37797814, 0.28001309],\n",
    "                 [0.37797814, 0.28001309, 0.34200877]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "\n",
    "jn(H_tn, Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Calcul des gradients\n",
    "\n",
    "La taille des gradients est la même que celle des paramètres $\\theta[N, L]$. \n",
    "\n",
    "$$\\frac{\\partial J}{\\theta_j} = \\frac{1}{M} \\sum\\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X^{(i)}_{j} $$\n",
    "\n",
    "Sa forme matricielle sera \n",
    "$$\\frac{\\partial J}{\\theta_j} = \\frac{1}{M} X^\\top \\cdot (H-Y) $$\n",
    "\n",
    "- $X[M, N]$ : une matrice de M lignes (échantillons) et N colonnes (caractéristiques, y compris le biais).  \n",
    "- $H[M, L]$ : les probabilités estimées de chaque échantillon (M) de chaque classe (L)\n",
    "- $Y[M, L]$ : les probabilités réelles (1 ou 0) de chaque échantillon (M) de chaque classe (L)\n",
    "- $\\frac{\\partial J}{\\theta}[N, L]$ : une matrice de L lignes (classes) et N colonnes (caractéristiques, y compris le biais). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06543131, -0.11961822,  0.18504953],\n",
       "       [-0.07000327,  0.16449781, -0.09449454]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Gradients multiclasses\n",
    "def dJn(X: np.ndarray, H: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "   \n",
    "    M=len(Y)\n",
    "    \n",
    "    return np.dot(X.transpose(),(H-Y))/M\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[-0.06543131, -0.11961822,  0.18504953],\n",
    "#        [-0.07000327,  0.16449781, -0.09449454]])\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "                 [0.36029662, 0.24151404, 0.39818934],\n",
    "                 [0.34200877, 0.37797814, 0.28001309],\n",
    "                 [0.37797814, 0.28001309, 0.34200877]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "\n",
    "dJn(X_tn, H_tn, Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.6. Descente du gradient adaptative\n",
    "\n",
    "**Rien à programmer ici**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descente(X, Y, Theta, ITER=100, alpha=0.1):\n",
    "    couts = []\n",
    "\n",
    "    Theta = Theta.copy() # pour ne pas modifier Theta original\n",
    "    \n",
    "    for i in range(ITER): # Ici, la seule condition d'arrêt est le nombre des itérations\n",
    "        H = softmax(zfn(X, Theta))\n",
    "        couts.append(jn(H, Y))\n",
    "        Theta = Theta - alpha * dJn(X, H, Y)\n",
    "    \n",
    "    return Theta, couts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.7. Regrouper les fonctions ensemble \n",
    "\n",
    "Pour bien gérer l'entraînement et la prédiction, les fonctions que nous avions implémentées sont regroupées dans une seul classe. \n",
    "L'intérêt : \n",
    "- Si nous appliquons la normalisation durant l'entraînement, nous devons l'appliquer aussi durant la prédiction. En plus, nous devons utiliser les mêmes paramètres (moyenne et écart-type)\n",
    "- Nous utilisons les thétas optimales lors de la prédicition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normaliser(X, mean=None, std=None): \n",
    "    if (mean is None) or (std is None): \n",
    "        mean = np.mean(X, axis=0)\n",
    "        std = np.std(X, axis=0)\n",
    "    X_norm = np.where(std==0, X, (X - mean)/std)\n",
    "    return X_norm, mean, std\n",
    "\n",
    "def preparer(X, norm=True, const=True, mean=None, std=None): \n",
    "    X_pre = X.copy()\n",
    "    if norm: \n",
    "        X_pre, mean, std = normaliser(X_pre,mean=mean, std=std)\n",
    "    if const:\n",
    "        X_pre = np.append(np.ones((X_pre.shape[0],1)), X_pre ,axis=1)\n",
    "    return X_pre, mean, std\n",
    "\n",
    "class MaxEnt(object):\n",
    "    \n",
    "    def __init__(self, norm=True, const=True): \n",
    "        self.norm = norm\n",
    "        self.const = const\n",
    "    \n",
    "    def entrainer(self, X, Y, max_iter=100, alpha=.01): \n",
    "        X_pre, self.mean, self.std = preparer(X, norm=self.norm, const=self.const)\n",
    "        Theta = np.zeros((X_pre.shape[1], Y.shape[1])) # Theta[N, L]\n",
    "        self.Theta, self.couts = descente(X_pre, Y, Theta, ITER=max_iter, alpha=alpha)\n",
    "        \n",
    "        \n",
    "    # La prédiction\n",
    "    # si prob=True elle rend un vecteur de probabilités\n",
    "    # sinon elle rend une vecteur de 1 et 0\n",
    "    def predire(self, X, prob=True):\n",
    "        X_pre, self.mean, self.std = preparer(X, norm=self.norm, const=self.const, mean=self.mean, std=self.std)\n",
    "        H = softmax(zfn(X_pre, self.Theta))\n",
    "        if prob:\n",
    "            return H\n",
    "        return cn(H)\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[1, 0, 0],\n",
    "#        [0, 1, 0],\n",
    "#        [0, 1, 0],\n",
    "#        [0, 0, 1]])\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]]) # deux variables logiques\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]]) # égale, sup, inf, égale\n",
    "\n",
    "X_testn = np.array([[2., 2.], [1., 0.], [1., -1.], [2., 5.]])\n",
    "\n",
    "maxent = MaxEnt()\n",
    "maxent.entrainer(X_tn, Y_tn)\n",
    "maxent.predire(X_testn, prob=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "Nous allons utiliser [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) pour classer des fleurs en trois classes, en utilisant 4 caractéristiques. \n",
    "Pour simplification, nous allons utiliser seulement 2 caractéristiques : Petal Length (cm); Petal Width (cm). \n",
    "D'après [Ce tutoriel](https://teddykoker.com/2019/06/multi-class-classification-with-logistic-regression-in-python/) ces deux caractéristiques sont suffisantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv('data/iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petal_length  petal_width        class\n",
       "0           1.4          0.2  Iris-setosa\n",
       "1           1.4          0.2  Iris-setosa\n",
       "2           1.3          0.2  Iris-setosa\n",
       "3           1.5          0.2  Iris-setosa\n",
       "4           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if iris.shape[1] > 3:\n",
    "    iris.drop(['sepal_length', 'sepal_width'], axis = 1, inplace=True)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>body fat_%</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>gripForce</th>\n",
       "      <th>sit and bend forward_cm</th>\n",
       "      <th>sit-ups counts</th>\n",
       "      <th>broad jump_cm</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>172.3</td>\n",
       "      <td>75.24</td>\n",
       "      <td>21.3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>54.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>55.80</td>\n",
       "      <td>15.7</td>\n",
       "      <td>77.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>53.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.0</td>\n",
       "      <td>179.6</td>\n",
       "      <td>78.00</td>\n",
       "      <td>20.1</td>\n",
       "      <td>92.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>174.5</td>\n",
       "      <td>71.10</td>\n",
       "      <td>18.4</td>\n",
       "      <td>76.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>41.4</td>\n",
       "      <td>15.2</td>\n",
       "      <td>53.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>173.8</td>\n",
       "      <td>67.70</td>\n",
       "      <td>17.1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>43.5</td>\n",
       "      <td>27.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  height_cm  weight_kg  body fat_%  diastolic  systolic  gripForce  \\\n",
       "0  27.0      172.3      75.24        21.3       80.0     130.0       54.9   \n",
       "1  25.0      165.0      55.80        15.7       77.0     126.0       36.4   \n",
       "2  31.0      179.6      78.00        20.1       92.0     152.0       44.8   \n",
       "3  32.0      174.5      71.10        18.4       76.0     147.0       41.4   \n",
       "4  28.0      173.8      67.70        17.1       70.0     127.0       43.5   \n",
       "\n",
       "   sit and bend forward_cm  sit-ups counts  broad jump_cm class  \n",
       "0                     18.4            60.0          217.0     C  \n",
       "1                     16.3            53.0          229.0     A  \n",
       "2                     12.0            49.0          181.0     C  \n",
       "3                     15.2            53.0          219.0     B  \n",
       "4                     27.1            45.0          217.0     B  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# https://www.kaggle.com/datasets/kukuroo3/body-performance-data\n",
    "body = pd.read_csv('data/bodyPerformance.csv')\n",
    "\n",
    "# transformer le sex en un vecteur de deux elements et supprimer le\n",
    "gender = OneHotEncoder().fit_transform(body[['gender']]).toarray()\n",
    "body.drop(['gender'], axis=1, inplace=True)\n",
    "body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10714, 12), (2679, 12))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xbody = body.iloc[:, :-1].values # Premières colonnes \n",
    "\n",
    "# Ajouter le sex encodé aux caractéristiques\n",
    "Xbody = np.concatenate((Xbody, gender), axis=1)\n",
    "\n",
    "Ybody = body.iloc[:,  -1].values # Dernière colonne   \n",
    "\n",
    "Xbody_train, Xbody_test, Ybody_train, Ybody_test = train_test_split(Xbody, Ybody, \n",
    "                                                                    test_size   =0.2, # 20% pour le teste\n",
    "                                                                    random_state=0, \n",
    "                                                                    stratify    =Ybody) # stratification sur Yiris\n",
    "\n",
    "\n",
    "Xbody_train.shape, Xbody_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Séparabilité des classes\n",
    "\n",
    "Ici, nous allons vérifier la séparabilité des classes visuellement (en se basant sur les deux caractéristiques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xiris = iris.iloc[:, :-1].values # Premières colonnes \n",
    "\n",
    "Yiris = iris.iloc[:,  -1].values # Dernière colonne \n",
    "\n",
    "setosa     = iris['class'] == 'Iris-setosa'\n",
    "versicolor = iris['class'] == 'Iris-versicolor'\n",
    "virginica  = iris['class'] == 'Iris-virginica'\n",
    "\n",
    "plt.scatter(Xiris[setosa,     0], Xiris[setosa,     1], color='red'  , marker='o', label='Iris-setosa'    )\n",
    "plt.scatter(Xiris[versicolor, 0], Xiris[versicolor, 1], color='blue' , marker='v', label='Iris-versicolor')\n",
    "plt.scatter(Xiris[virginica,  0], Xiris[virginica,  1], color='green', marker='*', label='Iris-virginica' )\n",
    "\n",
    "plt.xlabel('sepal_length')\n",
    "plt.ylabel('sepal_width' )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous concernant la séparabilité des 3 classes?\n",
    "- Donner une hypothèse concernant la performance d'un modèle de classement comme la régression logistique sur ce dataset (Rappel, Précision)\n",
    "- Justifier cette hypothèse (Rappel, Précision) en comparant les 3 classes\n",
    "\n",
    "**Réponse**\n",
    "- D'après la visualisation obtenue, on peut remarquer que la séparabilité des classes 'Iris-setosa' est assez élevée, tandis que les classes 'Iris-versicolor' et 'Iris-virginica' se chevauchent un peu. \n",
    "- Par conséquent, on peut faire l'hypothèse qu'un modèle de classification tel que la régression logistique pourrait bien performer sur ce jeu de données, mais pourrait avoir des difficultés à distinguer les classes 'Iris-versicolor' et 'Iris-virginica'.\n",
    "- Nous pouvons justifier l'hypothèse selon laquelle un modèle de classification tel que la régression logistique pourrait bien performer sur le jeu de données Iris pour la classe 'Iris-setosa', mais pourrait avoir des difficultés à distinguer les classes 'Iris-versicolor' et 'Iris-virginica' en raison de leur chevauchement.\n",
    "        \n",
    "        En termes de Rappel et Précision, cela signifie que le modèle aura un rappel élevé (nombre de vrais positifs divisé par le nombre total de vrais positifs et de faux négatifs) pour la classe \"Iris-setosa\", car il sera capable de détecter la plupart des exemples positifs, mais un rappel moins élevé pour les autres classes. \n",
    "        \n",
    "        En même temps, le modèle aura une précision élevée (nombre de vrais positifs divisé par le nombre total de prédictions positives) pour la classe \"Iris-setosa\", car il fera relativement peu d'erreurs de prédiction pour cette classe, mais une précision moins élevée pour les autres classes en raison de l'augmentation des faux positifs et des faux négatifs.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xiris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13136\\2435991775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m Xiris_train, Xiris_test, Yiris_train, Yiris_test = train_test_split(Xiris, Yiris, \n\u001b[0m\u001b[0;32m      4\u001b[0m                                                                     \u001b[0mtest_size\u001b[0m   \u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# 20% pour le teste\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                                     \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xiris' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "Xiris_train, Xiris_test, Yiris_train, Yiris_test = train_test_split(Xiris, Yiris, \n",
    "                                                                    test_size   =0.2, # 20% pour le teste\n",
    "                                                                    random_state=0, \n",
    "                                                                    stratify    =Yiris) # stratification sur Yiris\n",
    "\n",
    "len(Xiris_train), len(Xiris_test), np.unique(Yiris_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. One-vs-Rest OU MaxEnt\n",
    "\n",
    "Nous avons entrainé deux modèles : \n",
    "1. **One-vs-Rest** : ici, trois sous-modèles binaires sont entraînés ; un pour chaque class. Chaque sous modèle détecte si l'échantillon appartient à sa classe ou non. Lors de la prédiction, on prend la classe avec le max de probabilité\n",
    "1. **MaxEnt** : ici, un modèle de régression logistique multinomiale (maximum entropy) est entraîné pour séparer les trois classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-Rest\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        10\n",
      "Iris-versicolor       0.91      1.00      0.95        10\n",
      " Iris-virginica       1.00      0.90      0.95        10\n",
      "\n",
      "       accuracy                           0.97        30\n",
      "      macro avg       0.97      0.97      0.97        30\n",
      "   weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "MaxEnt\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        10\n",
      "Iris-versicolor       1.00      1.00      1.00        10\n",
      " Iris-virginica       1.00      1.00      1.00        10\n",
      "\n",
      "       accuracy                           1.00        30\n",
      "      macro avg       1.00      1.00      1.00        30\n",
      "   weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "one2rest = LogisticRegression(solver='lbfgs', penalty=\"none\", multi_class='ovr'        )\n",
    "one2one  = LogisticRegression(solver='lbfgs', penalty=\"none\", multi_class='multinomial')\n",
    "\n",
    "one2rest.fit(Xiris_train, Yiris_train)\n",
    "one2one .fit(Xiris_train, Yiris_train)\n",
    "\n",
    "print('One-vs-Rest')\n",
    "print(classification_report(Yiris_test, one2rest.predict(Xiris_test)))\n",
    "\n",
    "print('MaxEnt')\n",
    "print(classification_report(Yiris_test, one2one.predict(Xiris_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "\n",
    "Nous remarquons que la performance de MaxEnt est meilleure que celle de One-vs-Rest\n",
    "- Pourquoi ? (en se basant sur la limite de décision et les paramètres)\n",
    "- Quelle est l'approche (parmi ces deux) qui est affectée beaucoup plus par les valeurs aberrantes (les échantillons d'une classe qui peuvent se retrouver aux milieu d'une autre classe)\n",
    "\n",
    "**Réponse**\n",
    "- La différence de performance peut être due à la façon dont les deux modèles abordent la séparation des classes.Dans One-vs-Rest, chaque classe est séparée du reste des échantillons, ce qui peut être difficile à réaliser lorsque les classes se chevauchent. Dans le cas de MaxEnt, un modèle multinomial est utilisé pour séparer directement les trois classes en même temps, ce qui peut être plus efficace pour des données telles que celles de l'Iris.\n",
    "- L'approche One-vs-Rest est plus affectée par les valeurs aberrantes que l'approche MaxEnt car  One-vs-Rest entraîne plusieurs modèles binaires indépendants pour chaque classe, donc les points d'une classe qui se retrouvent au milieu d'une autre classe peuvent fausser la classification de plusieurs modèles binaires différents, alors que MaxEnt utilise un seul modèle pour séparer les trois classes, ce qui peut être plus robuste aux valeurs aberrantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3. One-vs-Rest OU One-vs-One\n",
    "\n",
    "Nous avons entrainé deux modèles : \n",
    "1. One-vs-Rest\n",
    "1. One-vs-One\n",
    "\n",
    "Les deux modèles sont comparés en se basant sur plusieurs critères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://goshippo.com/blog/measure-real-size-any-python-object/\n",
    "import sys\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13136\\1737599658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mConvergenceWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0movr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0movo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneVsOneClassifier\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "import sys, timeit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ce block du code est pour filtrer les avertissements concernant la convergence du modèle\n",
    "# en général, lorsque e nombre des itérations n'est pas suffisant pour atteindre l'erreur minimale\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "ovr = OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter=100, penalty=None, n_jobs=1))\n",
    "ovo = OneVsOneClassifier (LogisticRegression(solver='lbfgs', max_iter=100, penalty=None, n_jobs=1))\n",
    "\n",
    "temps_train = []\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "ovr.fit(Xbody_train, Ybody_train)\n",
    "temps_train.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "ovo.fit(Xbody_train, Ybody_train)\n",
    "temps_train.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_test = []\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "ovr_res = ovr.predict(Xbody_test)\n",
    "temps_test.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "ovo_res = ovo.predict(Xbody_test)\n",
    "temps_test.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "taille = [get_size(ovr), get_size(ovo)]\n",
    "\n",
    "accuracy = [\n",
    "    accuracy_score(Ybody_test, ovr_res),\n",
    "    accuracy_score(Ybody_test, ovo_res)\n",
    "]\n",
    "\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Algorithme'            : ['OvR', 'OvO']  ,\n",
    "    'Taille'                : taille,\n",
    "    'Temps d\\'entrainement' : temps_train,\n",
    "    'Temps de test'         : temps_test,\n",
    "    'Accuracy'              : accuracy,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "\n",
    "- Pourquoi la taille OvO est plus grande que celle de OvR ?\n",
    "- Le temps d'entrainement est différent d'une exécution à une autre. Quand est-ce que un modèle est plus rappide que l'autre ?\n",
    "- Par contre, le temps de test OvO est plus lourd. Pourquoi?\n",
    "- Si nous pouvons paralleliser tous les modèles binaires, quel est l'effet sur les deux ?\n",
    "- Pourquoi OvO généralise-t-il mieux que OvR ?\n",
    "- Pouvons-nous utiliser OvR pour multi-label ? Pourquoi ?\n",
    "- Pouvons-nous utiliser OvO pour multi-label ? Pourquoi ?\n",
    "\n",
    "**Réponse**\n",
    " - La taille de OvO est plus grande que celle de OvR car OvO doit entraîner un nombre de modèles binaires proportionnel au carré du nombre de classes, tandis que OvR ne nécessite qu'un nombre de modèles égal au nombre de classes.\n",
    "    Autrement dit : \n",
    "        \n",
    "        Dans OvO: on construit un classificateur binaire pour chaque paire de classes distincts , donc on aura : (N * (N-1) / 2 )  classificateurs binaires  , où N est le nombre de classes. \n",
    "        \n",
    "        Dans OvR: ON construit un classificateur binaire pour chaque classe par rapport a tous les autres classes restantes,donc on aura : N classificateur binaire seulement  où N est toujours le nombre de classes.  \n",
    "    \n",
    "    \n",
    "-  Le temps d'entrainement est différent:\n",
    "\n",
    "         Cas1 :OvR est plus rapide que OvO lorsque le nombre de classes est élevé et que la taille de l'ensemble de données est grande\n",
    "         Cas2 :Lorsque le nombre de classes est faible et que la taille de l'ensemble de données est petite, OvO est plus rapide qu'OvR.\n",
    "       \n",
    "       \n",
    "- Le temps de test d'OvO est plus lourd que celui d'OvR parce que\n",
    "\n",
    "        OvO nécessite de faire plus de prédictions pour chaque instance de test. Avec OvO, il faut utiliser plusieurs classificateurs binaires pour prédire la classe de l'instance, tandis qu'avec OvR, il suffit d'utiliser un seul classificateur binaire pour chaque instance. Donc, cela prend plus de temps pour OvO pour effectuer toutes les prédictions nécessaires.\n",
    "\n",
    "\n",
    "\n",
    "-  L'effet de la   parallelisation sur les deux \n",
    " \n",
    " \n",
    "          Accélérer le temps d'entraînement en effectuant des calculs en parallèle sur plusieurs processeurs\n",
    "          Améliorer la scalabilité , car les deux approches       peuvent traiter plus rapidement de plus grands ensembles de données ou faire des prédictions plus rapidement.\n",
    "\n",
    "- La raison pour laquelle la méthode OvO généralise souvent mieux que la méthode OvR  est que \n",
    "\n",
    "        chaque classificateur binaire dans la méthode OvO est entraîné sur un sous-ensemble plus petit de l'ensemble de données, composé uniquement des exemples appartenant à deux classes. Cela permet à chaque classificateur binaire de se concentrer sur les différences subtiles entre deux classes plutôt que de confondre plusieurs classes dans la méthode OvR.\n",
    "\n",
    "- Oui,on peut utiliser OvR pour la classification multi-label car \n",
    "         \n",
    "         on peut  entraîner un classificateur binaire pour chaque étiquette par rapport à toutes les autres étiquettes,mais cela ne prend pas en compte les interactions entre les étiquettes.\n",
    "         \n",
    "- Oui,on peut utiliser OvO car \n",
    "\n",
    "        on peut  entraîner un classificateur binaire pour chaque paire de combinaisons d'étiquettes possibles  pour la classificatio multi-label mais cela peut être coûteux en temps calcul pour un grand nombre d'étiquettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FIN du TP ... Enfin! :)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'FIN du TP ... Enfin! :)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
